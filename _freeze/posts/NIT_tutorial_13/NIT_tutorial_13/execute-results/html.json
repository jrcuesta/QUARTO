{
  "hash": "ae9348ac2d16b928d8f93358a6ce010e",
  "result": {
    "markdown": "---\ntitle: \"PLS - NIT tutorial (part 13)\"\nauthor: \"José Ramón Cuesta\"\ndescription: \"Developing our firsts PLS Models\"\nimage: Moi_pred.jpeg\ndate: \"2023-01-09\"\ndraft: false\ncategories: [R, NIT Tutorial, PLS]\n---\n\n\n## Splitting the data before the calibration.\n\nIn this post we are going to use the package Caret to develop PLS models for protein, fat and moisture, but first we have to split the data in two sets, one for training and other for testing. Now we load the libraries we need, and the workspace from the previous post.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"C:/BLOG/Workspaces/NIT Tutorial/NIT_ws12.RData\")\nlibrary(tidymodels)\nlibrary(caret)\n```\n:::\n\n\nWe did use Caret to split the tecator data in the post NIT_tutorial_5, so we repeat the process again with our last dataframe (from the previous post \"tecator2\"):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\ntecator2_split_prot <- initial_split(tecator2, prop = 3/4, strata = Protein)\ntecator2_split_fat <- initial_split(tecator2, prop = 3/4, strata = Fat)\ntecator2_split_moi <- initial_split(tecator2, prop = 3/4, strata = Moisture)\n\ntec2_prot_train <- training(tecator2_split_prot)\ntec2_prot_test <- testing(tecator2_split_prot)\n\ntec2_fat_train <- training(tecator2_split_fat)\ntec2_fat_test <- testing(tecator2_split_fat)\n\ntec2_moi_train <- training(tecator2_split_moi)\ntec2_moi_test <- testing(tecator2_split_moi)\n```\n:::\n\n\nWe have done this in a random way, so we expect that some of the mahalanobis distance outliers samples have gone to the training set and the remaining to the test set. Now, we can start to develop the regressions with the training set, and the validation with the test set, using the Caret package with the \"pls\" algorithm. We will use the cross validation (10 folds), to select the number of terms.\n\n## Model and validation performance for Protein\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nctrl <- trainControl(method = \"cv\", number = 10)\ntec2_pls_model_prot <- train(y = tec2_prot_train$Protein,\n  x = tec2_prot_train$snvdt2der2_spec,\n  method = \"pls\",\n  trControl = ctrl)\n\n\ntec2_pls_model_prot\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPartial Least Squares \n\n160 samples\n100 predictors\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 144, 144, 144, 144, 144, 143, ... \nResampling results across tuning parameters:\n\n  ncomp  RMSE      Rsquared   MAE      \n  1      1.451693  0.7398494  1.0940694\n  2      1.342024  0.7854652  1.0083106\n  3      1.301409  0.8228018  0.9822213\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was ncomp = 3.\n```\n:::\n:::\n\n\nnow we can see how the model performs with the test set, and create a table with the \"SampleID\n\n\n::: {.cell}\n\n```{.r .cell-code}\npls_preds <- predict(tec2_pls_model_prot, tec2_prot_test$snvdt2der2_spec)\ntest_prot_preds <- bind_cols(tec2_prot_test$SampleID ,tec2_prot_test$Protein, pls_preds, tec2_prot_test$outlier)\ncolnames(test_prot_preds) <- c(\"SampleID\", \"Prot_lab\", \"Prot_pred\", \"Outlier\")\n```\n:::\n\n\nan XY plot (laboratory vs. predicted values) can gives an idea of the performance of the validation test:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_prot_preds %>% \n  ggplot(aes(x = Prot_lab, y = Prot_pred, colour = Outlier)) +\n  geom_point(size = 3) +\n  geom_abline() +\n  scale_color_manual(values = c(\"no outlier\" = \"green\",\n                                \"Warning outlier\" = \"orange\",\n                                \"Action outlier\" =\"red\"))\n```\n\n::: {.cell-output-display}\n![Protein predicted vs reference values.](NIT_tutorial_13_files/figure-html/fig-plotprot-1.png){#fig-plotprot width=672}\n:::\n:::\n\n\n## Model and validation performance for Fat\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nctrl <- trainControl(method = \"cv\", number = 10)\ntec2_pls_model_fat <- train(y = tec2_fat_train$Fat,\n  x = tec2_fat_train$snvdt2der2_spec,\n  method = \"pls\",\n  trControl = ctrl)\n\n\ntec2_pls_model_fat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPartial Least Squares \n\n160 samples\n100 predictors\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 144, 145, 143, 144, 144, 144, ... \nResampling results across tuning parameters:\n\n  ncomp  RMSE      Rsquared   MAE     \n  1      2.339072  0.9731581  1.832527\n  2      2.276391  0.9762236  1.757150\n  3      2.215674  0.9782784  1.744476\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was ncomp = 3.\n```\n:::\n:::\n\n\nnow we can see how the model performs with the test set, and create a table with the \"SampleID\n\n\n::: {.cell}\n\n```{.r .cell-code}\npls_preds_fat <- predict(tec2_pls_model_fat, tec2_fat_test$snvdt2der2_spec)\ntest_fat_preds <- bind_cols(tec2_fat_test$SampleID ,tec2_fat_test$Fat, pls_preds_fat, tec2_fat_test$outlier)\ncolnames(test_fat_preds) <- c(\"SampleID\", \"Fat_lab\", \"Fat_pred\", \"Outlier\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_fat_preds %>% \n  ggplot(aes(x = Fat_lab, y = Fat_pred, colour = Outlier)) +\n  geom_point(size = 3) +\n  geom_abline() +\n  scale_color_manual(values = c(\"no outlier\" = \"green\",\n                                \"Warning outlier\" = \"orange\",\n                                \"Action outlier\" =\"red\"))\n```\n\n::: {.cell-output-display}\n![](NIT_tutorial_13_files/figure-html/plotfat-1.png){width=672}\n:::\n:::\n\n\n## Model and validation performance for Moisture\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nctrl <- trainControl(method = \"cv\", number = 10)\ntec2_pls_model_moi <- train(y = tec2_fat_train$Moisture,\n  x = tec2_fat_train$snvdt2der2_spec,\n  method = \"pls\",\n  trControl = ctrl)\n\n\ntec2_pls_model_moi\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPartial Least Squares \n\n160 samples\n100 predictors\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 144, 144, 143, 144, 144, 144, ... \nResampling results across tuning parameters:\n\n  ncomp  RMSE      Rsquared   MAE     \n  1      2.080841  0.9583109  1.641792\n  2      2.031741  0.9600832  1.593240\n  3      2.014572  0.9612094  1.566407\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was ncomp = 3.\n```\n:::\n:::\n\n\nnow we can see how the model performs with the test set, and create a table with the \"SampleID\n\n\n::: {.cell}\n\n```{.r .cell-code}\npls_preds_moi <- predict(tec2_pls_model_moi, tec2_moi_test$snvdt2der2_spec)\ntest_moi_preds <- bind_cols(tec2_moi_test$SampleID ,tec2_moi_test$Moisture, pls_preds_moi, tec2_moi_test$outlier)\ncolnames(test_moi_preds) <- c(\"SampleID\", \"Moi_lab\", \"Moi_pred\", \"Outlier\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_moi_preds %>% \n  ggplot(aes(x = Moi_lab, y = Moi_pred, colour = Outlier)) +\n  geom_point(size = 3) +\n  geom_abline() +\n  scale_color_manual(values = c(\"no outlier\" = \"green\",\n                                \"Warning outlier\" = \"orange\",\n                                \"Action outlier\" =\"red\"))\n```\n\n::: {.cell-output-display}\n![](NIT_tutorial_13_files/figure-html/plotmoi-1.png){width=672}\n:::\n:::\n\n\nNow I save the workspace to continue in the next post\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsave.image(\"C:/BLOG/Workspaces/NIT Tutorial/NIT_ws13.RData\")\n```\n:::\n\n\n## Table of results {style=\"color:blue\"}\n\n| Parameter | N training | N test | Terms | C.V. error (SECV) | Test Pred. error (SEV) |\n|------------|------------|------------|------------|------------|------------|\n| Protein   | 160        | 55     | 3     | 1.30              | 1.49                   |\n| Fat       | 160        | 55     | 3     | 2.22              | 2.34                   |\n| Moisture  | 160        | 56     | 3     | 2.01              | 2.25                   |\n\n## Conclussions {.illustration style=\"color:blue\"}\n\nWith all the plots above we can ask ourselves several questions in order to find the better performance we can get from the data.\n\n-   Can we force the model to use more terms , (few for the complexity of the data) , does this option improves the results on the test set?\n\n-   Can we use another model algorithm (different to \"pls\") to improve the results?.\n\n    -   do we have enough samples for those algorithms?\n\n-   Does the \"non MD outliers\" samples predict better than the \"warning\" or \"action\" MD outliers?\n\n-   Do we see non linearities in the plots?\n\n    -   How we can manages these non linearities?\n    -   Maybe adding more terms to the PLS model we improve the linearity.\n\n-   Comment any other conclusions you have.\n",
    "supporting": [
      "NIT_tutorial_13_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}